{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Lab4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooxZAq35AYiW"
      },
      "source": [
        "# COLX 563 Lab Assignment 4: Slot filling\n",
        "## Assignment Objectives\n",
        "\n",
        "In this lab, you will build an end-to-end system for basic (binary) intent recognition and slot filling in the context of a dialogue system. It is a team assignment, and you have nearly complete freedom with regards to your solution, with a few restrictions mentioned below. For this lab, you will work with your capstone team."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl5ClMaMAYie"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "Add imports below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N1xTcXVAYig",
        "outputId": "8591210c-b919-4e47-8f62-f33110e32c43"
      },
      "source": [
        "! pip install transformers\n",
        "! pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbmpYFAwHGul"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "from transformers import *\n",
        "import pandas as pd\n",
        "from itertools import cycle\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm, trange\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqFOYR5_AYih"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNwkKTAPAYih",
        "outputId": "d90c4564-8d69-4693-de3e-8f4cffb8ad49"
      },
      "source": [
        "manual_seed = 11\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if n_gpu > 0:\n",
        "    torch.cuda.manual_seed(manual_seed)\n",
        "\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Tesla V100-SXM2-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7qvw-xVAYih"
      },
      "source": [
        "For this lab, you'll be working with the MultiWOZ dataset of goal-oriented dialogues (2.2). You can look at the full corpus [here](https://github.com/budzianowski/multiwoz/tree/master/data/MultiWOZ_2.2). It has an impressively detailed annotation involving multiple turns and multiple goals which we have simplified to just the initiating request (first turn) and involving two possible intents and the corresponding slots for those intents. Download the data from [github](https://github.ubc.ca/jungyeul/COLX_563_adv-semantics_lab_students/raw/master/Multiwoz.zip), unzip it into a directory outside of your lab repo and change the path below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvcBHnUrGk8V"
      },
      "source": [
        "## Set up directories and uploead data. Ignore this cell if you are using data in the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOjwZkYqAYih"
      },
      "source": [
        "import os\n",
        "woz_directory =\"../content/data/Multiwoz/\"\n",
        "data_directory = \"../content/data/\"\n",
        "if os.path.exists(woz_directory) == False:\n",
        "    os.makedirs(woz_directory)\n",
        "if os.path.exists(data_directory) == False:\n",
        "    os.makedirs(data_directory)\n",
        "\n",
        "# train, dev, test data dir\n",
        "train_path = data_directory + \"train/\"\n",
        "if os.path.exists(train_path) == False:\n",
        "    os.makedirs(train_path)\n",
        "dev_path = data_directory + \"dev/\"\n",
        "if os.path.exists(dev_path) == False:\n",
        "    os.makedirs(dev_path)\n",
        "test_path = data_directory + \"test/\"\n",
        "if os.path.exists(test_path) == False:\n",
        "    os.makedirs(test_path)\n",
        "\n",
        "# output dir\n",
        "output_dir = data_directory + 'mtl-rb/'\n",
        "if os.path.exists(output_dir) == False:\n",
        "    os.makedirs(output_dir)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBMBtsF4AYii"
      },
      "source": [
        "## Tidy Submission\n",
        "rubric={mechanics:1}\n",
        "\n",
        "To get the marks for tidy submission:\n",
        "- Submit the assignment by filling in this Jupyter notebook with your answers embedded\n",
        "- Be sure to follow the instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlzbR_jRAYii"
      },
      "source": [
        "## Inspecting the data\n",
        "\n",
        "Let's look at corresponding pairs of utterances and answers from the training portion of our corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghsyOEHDAYii",
        "outputId": "6dfe9bba-ccf9-4f29-caa6-c85da2ea160b"
      },
      "source": [
        "count = 0\n",
        "with open(woz_directory + \"WOZ_train_utt.txt\") as f1:\n",
        "    with open(woz_directory + \"WOZ_train_ans.txt\") as f2:\n",
        "        while count < 20:\n",
        "            print(f1.readline().strip())\n",
        "            print(f2.readline().strip())\n",
        "            print(\"------\")\n",
        "            count += 1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Guten Tag, I am staying overnight in Cambridge and need a place to sleep. I need free parking and internet.\n",
            "find_hotel|hotel-area=centre|hotel-internet=yes|hotel-parking=yes\n",
            "------\n",
            "Hi there! Can you give me some info on Cityroomz?\n",
            "find_hotel|hotel-name=cityroomz\n",
            "------\n",
            "I am looking for a hotel named alyesbray lodge guest house.\n",
            "find_hotel|hotel-name=alyesbray lodge guest house\n",
            "------\n",
            "I am looking for a restaurant. I would like something cheap that has Chinese food.\n",
            "find_restaurant|restaurant-food=chinese|restaurant-pricerange=cheap\n",
            "------\n",
            "I'm looking for an expensive restaurant in the centre if you could help me.\n",
            "find_restaurant|restaurant-area=centre|restaurant-pricerange=expensive\n",
            "------\n",
            "I'm looking for a places to go and see during my upcoming trip to Cambridge.\n",
            "find_hotel\n",
            "------\n",
            "Yeah, could you recommend a good gastropub?\n",
            "find_restaurant|restaurant-food=gastropub\n",
            "------\n",
            "I want to find an expensive restaurant and serves european food. Can i also have the address, phone number and its area. ?\n",
            "find_restaurant|restaurant-food=european|restaurant-pricerange=expensive\n",
            "------\n",
            "Where's a good place to eat crossover food in Cambridge?\n",
            "find_restaurant|restaurant-food=crossover\n",
            "------\n",
            "I need a place to stay that has free wifi.\n",
            "find_hotel|hotel-internet=yes\n",
            "------\n",
            "I am looking for a restaurant that is in the expensive price range and in the south part of town.\n",
            "find_restaurant|restaurant-area=south|restaurant-pricerange=expensive\n",
            "------\n",
            "Can you help me find an expensive Chinese food restaurant?\n",
            "find_restaurant|restaurant-food=chinese|restaurant-pricerange=expensive\n",
            "------\n",
            "I'm looking to go to dinner tonight and am in the mood for some good Bistro in the centre of town, can you find me some options?\n",
            "find_restaurant|restaurant-area=centre|restaurant-food=bistro\n",
            "------\n",
            "I am looking for a particular restaurant. Its name is called travellers rest\n",
            "find_restaurant|restaurant-name=travellers rest\n",
            "------\n",
            "Heya, can you find me an expensive restaurant with north african food?\n",
            "find_restaurant|restaurant-food=african|restaurant-pricerange=expensive\n",
            "------\n",
            "I am looking for a restaurant.\n",
            "find_restaurant\n",
            "------\n",
            "I am looking for a high priced hotel in the north side of town\n",
            "find_hotel|hotel-area=north|hotel-pricerange=expensive\n",
            "------\n",
            "Hi, what can you tell me about the bangkok city restaurant?\n",
            "find_restaurant|restaurant-name=bangkok city\n",
            "------\n",
            "I'm looking for a hotel, can you help?\n",
            "find_hotel|hotel-type=hotel\n",
            "------\n",
            "Please find me a place to dine that's expensive and in the centre.\n",
            "find_restaurant|restaurant-area=centre|restaurant-pricerange=expensive\n",
            "------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYUVANxxAYij"
      },
      "source": [
        "The utterances consists of a request for information about either hotels or restaurants. The first part of the answer starts with the intent (either find_restaurant or find_hotel) and then lists the slots that have been filled in based on the utterance. Your goal is to generate this string of intents and slots based purely on the utterance. A few things to note:\n",
        "\n",
        "* Not all slots are filled in, and sometimes there are no slots filled in at all (but there is always an intent).\n",
        "* There are a fixed number of slots for each intent, and they always appear in a particular order, when they are filled in\n",
        "* The slot values sometimes but do not always correspond to what appears in the utterance. For example, a mention of wanting wifi in the request becomes hotel-internet=yes.\n",
        "\n",
        "We will be evaluating based on exact duplication of the entire output string, so before you start coding a solution, you should look carefully at examples in the training set and make sure you understand all the different components of the output, and how they related to the input utterance. In particular, you should identify the various constituent parts of the task, and judge which are likely to be easy, and which are likely to be more difficult."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFYUDPiMPh0i"
      },
      "source": [
        "## Preprocessing"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAD-_0foG5R0"
      },
      "source": [
        "## Preprocessing: extract sub-aspects and generate tsv file for each task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYxdFrobAYik"
      },
      "source": [
        "def get_aspects(aspects):\n",
        "    result = []\n",
        "    for aspect in aspects:\n",
        "        result.append(aspect.split(\"=\")[0])\n",
        "    return result   \n",
        "\n",
        "\n",
        "def get_X(utt_file):\n",
        "    X = []\n",
        "    with open(woz_directory + utt_file) as f1:\n",
        "        for line in f1:\n",
        "            line = line.strip()\n",
        "            X.append(line)\n",
        "    return X\n",
        "\n",
        "\n",
        "def get_full_aspect_set(ans_file):\n",
        "    full_aspect_set = set()\n",
        "    y = []\n",
        "    with open(woz_directory + ans_file) as f2:\n",
        "        for line in f2:\n",
        "            line = line.strip()\n",
        "            line_lst = line.split('|')\n",
        "            aspects = get_aspects(line_lst[1:])\n",
        "            full_aspect_set.update(aspects)\n",
        "            y.append(aspects)\n",
        "    return y, full_aspect_set\n",
        "\n",
        "\n",
        "def get_sub_aspect_set(full_aspect_set):\n",
        "    sub_aspect_set = set()\n",
        "    for aspect in full_aspect_set:\n",
        "        sub_aspect_set.add(aspect.split(\"-\")[1])\n",
        "    return sub_aspect_set\n",
        "\n",
        "\n",
        "def get_sub_y(y, sub_aspect_lst):\n",
        "    result = []\n",
        "    for sub_aspect in sub_aspect_lst:\n",
        "        tmp = []\n",
        "        for tags in y:\n",
        "            not_found = True\n",
        "            for tag in tags:\n",
        "                if sub_aspect in tag:\n",
        "                    tmp.append(sub_aspect)\n",
        "                    not_found = False\n",
        "                    break\n",
        "            if not_found: \n",
        "                tmp.append(\"not\")\n",
        "        result.append(tmp)\n",
        "    return result\n",
        "\n",
        "\n",
        "def write_tsv(utt_file, ans_file=\"\", split_type = \"train\"):\n",
        "    X = get_X(utt_file)\n",
        "    print(len(X))\n",
        "    # write contexts file\n",
        "    with open(f'./data/{split_type}/contents.txt', 'w') as file:\n",
        "        for context in X:\n",
        "            file.write('%s\\n' % context)\n",
        "    if split_type!=\"test\":\n",
        "        y, full_aspect_set = get_full_aspect_set(ans_file)\n",
        "        sub_aspect_set = get_sub_aspect_set(full_aspect_set)\n",
        "        sub_aspect_lst = sorted(list(sub_aspect_set))\n",
        "        print(sub_aspect_lst)\n",
        "        all_sub_y = get_sub_y(y, sub_aspect_lst)\n",
        "        \n",
        "        for i, y_sub_aspect in enumerate(all_sub_y):\n",
        "            # write tag files\n",
        "            with open(f'./data/{split_type}/{sub_aspect_lst[i]}.txt', 'w') as file:\n",
        "                for tag in y_sub_aspect:\n",
        "                    file.write('%s\\n' % tag)\n",
        "            # write tsv files\n",
        "            with open(f\"./data/{split_type}/content_{sub_aspect_lst[i]}.tsv\", \"w\") as fout:\n",
        "                fout.write(\"content\\tlabel\\n\")\n",
        "                for content, tag in zip(X, y_sub_aspect):\n",
        "                    fout.write(content + \"\\t\" + tag + \"\\n\")\n",
        "    \n",
        "        return X, all_sub_y\n",
        "    else:\n",
        "        with open(f\"./data/{split_type}/content.tsv\", \"w\") as fout:\n",
        "            fout.write(\"content\\tlabel\\n\")\n",
        "            for content in X:\n",
        "                fout.write(content + \"\\t\" + \"not\" + \"\\n\")\n",
        "\n",
        "        return X"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3C24sK4SNhp",
        "outputId": "74c90860-38cc-4d61-9d55-114888f7622e"
      },
      "source": [
        "write_tsv(\"WOZ_train_utt.txt\", \"WOZ_train_ans.txt\")\n",
        "write_tsv(\"WOZ_dev_utt.txt\", \"WOZ_dev_ans.txt\", \"dev\")\n",
        "write_tsv(\"WOZ_test_utt.txt\", \"\", \"test\");"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3760\n",
            "['area', 'food', 'internet', 'name', 'parking', 'pricerange', 'stars', 'type']\n",
            "413\n",
            "['area', 'food', 'internet', 'name', 'parking', 'pricerange', 'stars', 'type']\n",
            "400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv9qd2-fHO92"
      },
      "source": [
        "## Dataset, Encoder, Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hck0Z0iEAYim"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    # initialization\n",
        "    def __init__(self, dataframe, tokenizer, max_len, lab2ind):\n",
        "        \"\"\"\n",
        "          dataframe: pandas DataFrame.\n",
        "          tokenizer: Hugginfance BERT/RoBERTa tokenizer\n",
        "          max_len: maximal length of input sequence\n",
        "          lab2ind: dictionary of label classes\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.comment_text = self.data.content\n",
        "        self.labels = self.data.label\n",
        "        self.max_len = max_len\n",
        "        self.lab2ind = lab2ind\n",
        "\n",
        "    # get the size of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.comment_text)\n",
        "\n",
        "    # generate sample by index\n",
        "    def __getitem__(self, index):\n",
        "        # get ith sample and label\n",
        "        comment_text = str(self.comment_text[index])\n",
        "        label = str(self.labels[index])\n",
        "\n",
        "        label = self.lab2ind[label]\n",
        "        # use encode_plus() of Transformers to tokenize and vectorize input seuqnce and covert it to tensors. \n",
        "        # this method truncate or pad sequence to the maximal length and then return pytorch tensors. \n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            comment_text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            return_tensors = \"pt\"\n",
        "        )\n",
        "        return {\n",
        "            'ids': inputs['input_ids'].squeeze(0),  # shape of input_ids: [1, max_length]\n",
        "            'masks': inputs['attention_mask'].squeeze(0), # shape of attention_mask: [1, max_length]\n",
        "            'targets': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1zhDLXzAYim"
      },
      "source": [
        "def regular_encode(file_path, tokenizer, lab2ind, shuffle=True, num_workers = 2, batch_size=64, maxlen = 32, mode = 'train'): \n",
        "    '''\n",
        "      file_path: path to your dataset file\n",
        "      tokenizer: tokenizer method\n",
        "      lab2ind: label-to-index dictionary\n",
        "      shuffle: shuffle the dataset or not\n",
        "      num_workers: a number of data processors\n",
        "      batch_size: the number of batch size\n",
        "      maxlen: maximal sequence length\n",
        "      mode: the type of dataset\n",
        "    '''\n",
        "    # if we are in train mode, we will load two columns (i.e., text and label).\n",
        "    if mode == 'train':\n",
        "        # Use pandas to load dataset, the dataset should be a tsv file where the first line is the header.\n",
        "        df = pd.read_csv(file_path, delimiter='\\t',header=0, names=['content','label'], encoding='utf-8', quotechar=None, quoting=3)\n",
        "    \n",
        "    # if we are in predict mode, we will load one column (i.e., text).\n",
        "    elif mode == 'predict':\n",
        "        df = pd.read_csv(file_path, delimiter='\\t',header=0, names=['content', 'label'])\n",
        "        \n",
        "    else:\n",
        "        print(\"the type of mode should be either 'train' or 'predict'. \")\n",
        "        return\n",
        "        \n",
        "    print(\"{} Dataset: {}\".format(file_path, df.shape))\n",
        "    # instantiate the dataset instance \n",
        "    custom_set = CustomDataset(df, tokenizer, maxlen,lab2ind)\n",
        "    num_samples = len(custom_set)\n",
        "    num_labels = len(lab2ind)\n",
        "    dataset_params = {'batch_size': batch_size, 'shuffle': shuffle, 'num_workers': num_workers}\n",
        "\n",
        "    batch_data_loader = DataLoader(custom_set, **dataset_params)\n",
        "    # return a data iterator\n",
        "    return batch_data_loader, num_samples, num_labels"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRTP3Y7nAYin"
      },
      "source": [
        "model_name_path = \"roberta-base\"\n",
        "\n",
        "max_seq_length = 64\n",
        "train_batch_size = 32\n",
        "eval_batch_size = 128\n",
        "hidden_size = 768\n",
        "\n",
        "lr = 2e-5\n",
        "max_grad_norm = 1.0\n",
        "warmup_proportion = 0.1\n",
        "num_train_epochs = 5"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIIUza75O5tX"
      },
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained(model_name_path)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouVFsO86AYin"
      },
      "source": [
        "task_names = ['area', 'food', 'internet', 'name', 'parking', 'pricerange', 'stars', 'type']\n",
        "all_lab2ind = [{'area': 0, 'not': 1},{'food': 0, 'not': 1},\n",
        "               {'internet': 0, 'not': 1},{'name': 0, 'not': 1},\n",
        "               {'parking': 0, 'not': 1},{'pricerange': 0, 'not': 1},\n",
        "               {'stars': 0, 'not': 1},{'type': 0, 'not': 1}]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MddIfs36AYin",
        "outputId": "e185fce1-60dc-4048-a000-0b3c74f001b1"
      },
      "source": [
        "train_loaders = []\n",
        "valid_loaders = []\n",
        "test_loaders = []\n",
        "data_sizes = []\n",
        "total_training_batch = 0\n",
        "for i, task in enumerate(task_names):\n",
        "    lab2ind = all_lab2ind[i]\n",
        "    ##############################\n",
        "    train_loader, num_samples, num_label  = regular_encode(os.path.join(train_path, f\"content_{task}.tsv\"), tokenizer, lab2ind, shuffle=True, batch_size=train_batch_size, maxlen = max_seq_length)\n",
        "    \n",
        "    data_sizes.append(num_samples)\n",
        "    total_training_batch += len(train_loader)\n",
        "    train_loaders.append(iter(train_loader))\n",
        "    \n",
        "    ##############################\n",
        "    valid_loader, _, _  = regular_encode(os.path.join(dev_path, f\"content_{task}.tsv\"), tokenizer, lab2ind, shuffle=False, batch_size=eval_batch_size, maxlen = max_seq_length)\n",
        "    valid_loaders.append(valid_loader)\n",
        "    \n",
        "    ###############################\n",
        "    test_loader, _, _  = regular_encode(os.path.join(test_path, \"content.tsv\"), tokenizer, lab2ind, shuffle=False, batch_size=eval_batch_size, maxlen = max_seq_length, mode=\"predict\")\n",
        "    test_loaders.append(test_loader)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "../content/data/train/content_area.tsv Dataset: (3760, 2)\n",
            "../content/data/dev/content_area.tsv Dataset: (413, 2)\n",
            "../content/data/test/content.tsv Dataset: (400, 2)\n",
            "../content/data/train/content_food.tsv Dataset: (3760, 2)\n",
            "../content/data/dev/content_food.tsv Dataset: (413, 2)\n",
            "../content/data/test/content.tsv Dataset: (400, 2)\n",
            "../content/data/train/content_internet.tsv Dataset: (3760, 2)\n",
            "../content/data/dev/content_internet.tsv Dataset: (413, 2)\n",
            "../content/data/test/content.tsv Dataset: (400, 2)\n",
            "../content/data/train/content_name.tsv Dataset: (3760, 2)\n",
            "../content/data/dev/content_name.tsv Dataset: (413, 2)\n",
            "../content/data/test/content.tsv Dataset: (400, 2)\n",
            "../content/data/train/content_parking.tsv Dataset: (3760, 2)\n",
            "../content/data/dev/content_parking.tsv Dataset: (413, 2)\n",
            "../content/data/test/content.tsv Dataset: (400, 2)\n",
            "../content/data/train/content_pricerange.tsv Dataset: (3760, 2)\n",
            "../content/data/dev/content_pricerange.tsv Dataset: (413, 2)\n",
            "../content/data/test/content.tsv Dataset: (400, 2)\n",
            "../content/data/train/content_stars.tsv Dataset: (3760, 2)\n",
            "../content/data/dev/content_stars.tsv Dataset: (413, 2)\n",
            "../content/data/test/content.tsv Dataset: (400, 2)\n",
            "../content/data/train/content_type.tsv Dataset: (3760, 2)\n",
            "../content/data/dev/content_type.tsv Dataset: (413, 2)\n",
            "../content/data/test/content.tsv Dataset: (400, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdDcjzqkH5Am"
      },
      "source": [
        "## CLS layer, train, evaluation, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4wV_BnNAYio"
      },
      "source": [
        "class CLS_LAYER(nn.Module):\n",
        "    def __init__(self, label_num, hidden_size):\n",
        "        super(CLS_LAYER, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.label_num = label_num\n",
        "        \n",
        "        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        # the output dimention is the number of classes in the task. \n",
        "        self.fc = nn.Linear(self.hidden_size, self.label_num)\n",
        "        # initialization\n",
        "        initial_module(self.dense)\n",
        "        initial_module(self.fc)\n",
        "  \n",
        "    def forward(self, pooler_output):\n",
        "        \n",
        "        x = self.dense(pooler_output)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc(x)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMm2jLw7AYio"
      },
      "source": [
        "def initial_module(module):\n",
        "    torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    torch.nn.init.constant_(module.bias, 0)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRZgO6hLAYio"
      },
      "source": [
        "class MT_BERT(nn.Module):\n",
        "    def __init__(self, model_name_path, classifier_layers):\n",
        "        super(MT_BERT, self).__init__()\n",
        "\n",
        "        self.bert_model = RobertaModel.from_pretrained(model_name_path)\n",
        "        self.classifiers = nn.ModuleList(classifier_layers)\n",
        "\n",
        "    def forward(self, input_ids, input_mask, task_id):\n",
        "        outputs = self.bert_model(input_ids = input_ids, attention_mask = input_mask)\n",
        "        pooler_output = outputs['pooler_output']\n",
        "        \n",
        "        # select classification module according to the task index\n",
        "        logits = self.classifiers[task_id](pooler_output)\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gKzwWOaAYio"
      },
      "source": [
        "def create_model(model_name_path, label_list, hidden_size):\n",
        "    # create a classification module for each task\n",
        "    classification_layers = [CLS_LAYER(len(task_label2ind), hidden_size) for task_label2ind in label_list]\n",
        "\n",
        "    model = MT_BERT(model_name_path, classification_layers)\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J-VecqAMvm1",
        "outputId": "8de3af5b-880f-44c4-ff95-8480a6526606"
      },
      "source": [
        "print(\"total number of training batches:\", total_training_batch)\n",
        "train_loaders = [cycle(it) for it in train_loaders]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total number of training batches: 944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALN5EPKzNdiV"
      },
      "source": [
        "def train(model, optimizer, scheduler, loss_func, data_sizes, num_per_epoch, train_loaders):\n",
        "    '''\n",
        "    model: multi-task model\n",
        "    optimizer: AdamW optimizer\n",
        "    scheduler: learning rate scheduler\n",
        "    loss_func: loss funtion\n",
        "    data_sizes: a list of sizes of training sets\n",
        "    num_per_epoch: training steps of each epoch\n",
        "    train_loaders: a list of training dataloaders\n",
        "    '''\n",
        "    model.train()\n",
        "\n",
        "    # record training losses of all the tasks\n",
        "    tr_loss = [0. for i in range(len(data_sizes))]\n",
        "\n",
        "    # At each step, we sample a training dataloader to generate a batch. \n",
        "    # The sampling probability is based on the size of training set of each task. \n",
        "    total_sample = sum(data_sizes)\n",
        "    probs = [p/total_sample for p in data_sizes]\n",
        "\n",
        "    task_id = 0\n",
        "    epoch = 0\n",
        "\n",
        "    for step in range(num_per_epoch):\n",
        "        # Select a training dataloader by the sampling probability. \n",
        "        task_id = np.random.choice(int(len(data_sizes)), p=probs)\n",
        "\n",
        "        # Generate batch of selected task.\n",
        "        batch = next(train_loaders[task_id])\n",
        "        \n",
        "         # load data batch\n",
        "        input_ids = batch['ids'].to(device)\n",
        "        input_mask = batch['masks'].to(device)\n",
        "        labels = batch['targets'].to(device)\n",
        "        \n",
        "        # forward\n",
        "        outputs = model(input_ids, input_mask, task_id)\n",
        "        loss = loss_func(outputs, labels)\n",
        "\n",
        "        # delete used variables to free GPU memory\n",
        "        del batch, input_ids, input_mask, labels\n",
        "        optimizer.zero_grad()\n",
        "            \n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) \n",
        "        scheduler.step()\n",
        "    \n",
        "        # free GPU memory\n",
        "        if device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return tr_loss"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQuFUOp_Ng75"
      },
      "source": [
        "def evaluate(model, iterator, loss_func, task_id):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            input_ids = batch['ids'].to(device)\n",
        "            input_mask = batch['masks'].to(device)\n",
        "            labels = batch['targets'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, input_mask, task_id)\n",
        "\n",
        "            loss = loss_func(outputs, labels)\n",
        "            # delete used variables to free GPU memory\n",
        "            del batch, input_ids, input_mask\n",
        "\n",
        "            # identify the predicted class for each example in the batch\n",
        "            probabilities, predicted = torch.max(outputs.cpu().data, 1)\n",
        "            # put all the true labels and predictions to two lists\n",
        "            #print(predicted)\n",
        "            #print(labels)\n",
        "            all_pred.extend(predicted)\n",
        "            all_label.extend(labels.cpu())\n",
        "    \n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "\n",
        "    return epoch_loss / len(iterator), accuracy, f1score"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS4CXvpFbDai"
      },
      "source": [
        "def create_optimizer_and_scheduler(model, num_training_steps, warmup_steps, learning_rate):\n",
        "    \"\"\"\n",
        "    Setup the optimizer and the learning rate scheduler.\n",
        "    num_training_steps: the number of training steps\n",
        "    warmup_steps: the number of warm-up steps\n",
        "    learning_rate: the peak learning rate\n",
        "    \"\"\"\n",
        "    optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate\n",
        "    )\n",
        "    \n",
        "    lr_scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, \n",
        "        num_warmup_steps=warmup_steps, \n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    return optimizer, lr_scheduler"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1A8XJ4SNk3C"
      },
      "source": [
        "model = create_model(model_name_path, all_lab2ind, hidden_size).to(device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7QUJr1zQgTJ"
      },
      "source": [
        "num_training_steps  = total_training_batch * num_train_epochs\n",
        "num_warmup_steps = num_training_steps * warmup_proportion"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncHUiqybayMn"
      },
      "source": [
        "optimizer, scheduler = create_optimizer_and_scheduler(model, num_training_steps, num_warmup_steps, lr)\n",
        "#loss_func = nn.NLLLoss()\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt5bBOyHAYip"
      },
      "source": [
        "## Train the model for each sub aspect "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6F7TnUZAYiq",
        "outputId": "e1610dfa-8657-4674-bd80-7d75ccc3df7f"
      },
      "source": [
        "all_result_acc_dev = defaultdict(list)\n",
        "all_result_loss_dev = defaultdict(list)\n",
        "all_result_f1_dev = defaultdict(list)\n",
        "\n",
        "if os.path.isdir(output_dir) == False:\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "for epoch in trange(num_train_epochs, desc=\"Epoch\"):\n",
        "    text_file = open(os.path.join(output_dir,\"results.txt\"), \"a\")\n",
        "    _ = train(model, optimizer, scheduler, loss_func, data_sizes, total_training_batch, train_loaders)  \n",
        "    \n",
        "    # Evaluate at end of each epoch and save the evaluation results to a txt file.\n",
        "    text_file.write(' Epoch [{}/{}]\\n'.format(epoch+1, num_train_epochs))\n",
        "\n",
        "    for i, task in enumerate(task_names): \n",
        "        val_loss, val_acc, val_f1 = evaluate(model, valid_loaders[i], loss_func, i)\n",
        "        \n",
        "        all_result_acc_dev[task].append(val_acc)\n",
        "        all_result_loss_dev[task].append(val_loss)\n",
        "        all_result_f1_dev[task].append(val_f1)\n",
        "\n",
        "\n",
        "        text_file.write(' Task {}:\\n Validation Accuracy: {:.6f}, Validation F1: {:.6f}\\n'.format(task, val_acc, val_f1))\n",
        "        print(' Task {}:\\n Validation Accuracy: {:.6f}, Validation F1: {:.6f}\\n'.format(task, val_acc, val_f1))\n",
        "\n",
        "    text_file.write(\"\\n\\n\")\n",
        "    text_file.close()\n",
        "\n",
        "    final_result = {}\n",
        "    final_result[\"all_result_acc_dev\"] = all_result_acc_dev\n",
        "    final_result[\"all_result_loss_dev\"] = all_result_loss_dev\n",
        "    final_result[\"all_result_f1_dev\"] = all_result_f1_dev\n",
        "\n",
        "    torch.save(final_result, os.path.join(output_dir, \"all_res.pt\"))\n",
        "    \n",
        "    # Create a model checkpoint at end of each epoch\n",
        "    if torch.cuda.device_count() <= 1:\n",
        "        state_dict_model = model.state_dict()\n",
        "    else:\n",
        "        state_dict_model = model.module.state_dict()\n",
        "\n",
        "    state = {\n",
        "    'epoch': epoch,\n",
        "    'state_dict': state_dict_model,\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'scheduler': scheduler.state_dict()\n",
        "    }\n",
        "    \n",
        "    torch.save(state, os.path.join(output_dir,\"mt{}_{}.pt\".format(len(task_names),str(epoch+1))))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Task area:\n",
            " Validation Accuracy: 0.987893, Validation F1: 0.984895\n",
            "\n",
            " Task food:\n",
            " Validation Accuracy: 0.992736, Validation F1: 0.990267\n",
            "\n",
            " Task internet:\n",
            " Validation Accuracy: 0.997579, Validation F1: 0.993443\n",
            "\n",
            " Task name:\n",
            " Validation Accuracy: 0.990315, Validation F1: 0.987065\n",
            "\n",
            " Task parking:\n",
            " Validation Accuracy: 0.995157, Validation F1: 0.987021\n",
            "\n",
            " Task pricerange:\n",
            " Validation Accuracy: 0.990315, Validation F1: 0.988727\n",
            "\n",
            " Task stars:\n",
            " Validation Accuracy: 1.000000, Validation F1: 1.000000\n",
            "\n",
            " Task type:\n",
            " Validation Accuracy: 0.924939, Validation F1: 0.847824\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 1/5 [02:18<09:12, 138.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Task area:\n",
            " Validation Accuracy: 0.990315, Validation F1: 0.987884\n",
            "\n",
            " Task food:\n",
            " Validation Accuracy: 0.987893, Validation F1: 0.983884\n",
            "\n",
            " Task internet:\n",
            " Validation Accuracy: 0.997579, Validation F1: 0.993443\n",
            "\n",
            " Task name:\n",
            " Validation Accuracy: 0.992736, Validation F1: 0.990267\n",
            "\n",
            " Task parking:\n",
            " Validation Accuracy: 0.997579, Validation F1: 0.993576\n",
            "\n",
            " Task pricerange:\n",
            " Validation Accuracy: 0.995157, Validation F1: 0.994339\n",
            "\n",
            " Task stars:\n",
            " Validation Accuracy: 1.000000, Validation F1: 1.000000\n",
            "\n",
            " Task type:\n",
            " Validation Accuracy: 0.929782, Validation F1: 0.859598\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 2/5 [04:31<06:50, 136.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Task area:\n",
            " Validation Accuracy: 0.987893, Validation F1: 0.984895\n",
            "\n",
            " Task food:\n",
            " Validation Accuracy: 0.992736, Validation F1: 0.990267\n",
            "\n",
            " Task internet:\n",
            " Validation Accuracy: 0.997579, Validation F1: 0.993443\n",
            "\n",
            " Task name:\n",
            " Validation Accuracy: 0.987893, Validation F1: 0.983562\n",
            "\n",
            " Task parking:\n",
            " Validation Accuracy: 0.997579, Validation F1: 0.993576\n",
            "\n",
            " Task pricerange:\n",
            " Validation Accuracy: 0.995157, Validation F1: 0.994339\n",
            "\n",
            " Task stars:\n",
            " Validation Accuracy: 0.997579, Validation F1: 0.992093\n",
            "\n",
            " Task type:\n",
            " Validation Accuracy: 0.917676, Validation F1: 0.846716\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 3/5 [06:45<04:31, 135.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Task area:\n",
            " Validation Accuracy: 0.987893, Validation F1: 0.984895\n",
            "\n",
            " Task food:\n",
            " Validation Accuracy: 0.992736, Validation F1: 0.990267\n",
            "\n",
            " Task internet:\n",
            " Validation Accuracy: 0.997579, Validation F1: 0.993443\n",
            "\n",
            " Task name:\n",
            " Validation Accuracy: 0.990315, Validation F1: 0.986894\n",
            "\n",
            " Task parking:\n",
            " Validation Accuracy: 0.995157, Validation F1: 0.987021\n",
            "\n",
            " Task pricerange:\n",
            " Validation Accuracy: 0.992736, Validation F1: 0.991527\n",
            "\n",
            " Task stars:\n",
            " Validation Accuracy: 1.000000, Validation F1: 1.000000\n",
            "\n",
            " Task type:\n",
            " Validation Accuracy: 0.932203, Validation F1: 0.859592\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 4/5 [08:59<02:15, 135.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Task area:\n",
            " Validation Accuracy: 0.987893, Validation F1: 0.984895\n",
            "\n",
            " Task food:\n",
            " Validation Accuracy: 0.992736, Validation F1: 0.990267\n",
            "\n",
            " Task internet:\n",
            " Validation Accuracy: 0.997579, Validation F1: 0.993443\n",
            "\n",
            " Task name:\n",
            " Validation Accuracy: 0.990315, Validation F1: 0.986894\n",
            "\n",
            " Task parking:\n",
            " Validation Accuracy: 0.995157, Validation F1: 0.987021\n",
            "\n",
            " Task pricerange:\n",
            " Validation Accuracy: 0.995157, Validation F1: 0.994339\n",
            "\n",
            " Task stars:\n",
            " Validation Accuracy: 1.000000, Validation F1: 1.000000\n",
            "\n",
            " Task type:\n",
            " Validation Accuracy: 0.920097, Validation F1: 0.848420\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 5/5 [11:12<00:00, 134.44s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW7zwHYYJWIX"
      },
      "source": [
        "## Predict and output sub_aspect_pred.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_xcJjWGJh3l"
      },
      "source": [
        "model = create_model(model_name_path, all_lab2ind, hidden_size)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_tmXtKVJjRr"
      },
      "source": [
        "checkpoint = torch.load(output_dir+\"mt8_4.pt\", map_location='cpu')\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "model = model.to(device)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2jSmVU2TBt_",
        "outputId": "32b75a8a-59f9-43ad-b2b1-a1d50cb54ae2"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 0.9322033898305084, 0.8595920349684313)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB0fnBEHIxFI"
      },
      "source": [
        "# task_names = ['area', 'food', 'internet', 'name', 'parking', 'pricerange', 'stars', 'type']\n",
        "def predict(model, iterator, task_id):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    all_pred=[]\n",
        "    #all_label = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            input_ids = batch['ids'].to(device)\n",
        "            \n",
        "            input_mask = batch['masks'].to(device)\n",
        "            labels = batch['targets'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, input_mask, task_id)\n",
        "\n",
        "            #loss = loss_func(outputs, labels)\n",
        "            # delete used variables to free GPU memory\n",
        "            del batch, input_ids, input_mask\n",
        "            \n",
        "            # identify the predicted class for each example in the batch\n",
        "            probabilities, predicted = torch.max(outputs.cpu().data, 1)\n",
        "            # put all the true labels and predictions to two lists\n",
        "            all_pred.extend(predicted)\n",
        "            #all_label.extend(labels.cpu())\n",
        "    \n",
        "    #accuracy = accuracy_score(all_label, all_pred)\n",
        "    #f1score = f1_score(all_label, all_pred, average='macro') \n",
        "\n",
        "    return all_pred"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etzr9boHA9Yv",
        "outputId": "44871b5f-9444-486b-d7e8-3989f96c1a89"
      },
      "source": [
        "all_ind2lab = []\n",
        "for task in all_lab2ind:\n",
        "    tmp = {}\n",
        "    for key,value in task.items():\n",
        "        tmp[value] = key\n",
        "    all_ind2lab.append(tmp)\n",
        "all_ind2lab"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{0: 'area', 1: 'not'},\n",
              " {0: 'food', 1: 'not'},\n",
              " {0: 'internet', 1: 'not'},\n",
              " {0: 'name', 1: 'not'},\n",
              " {0: 'parking', 1: 'not'},\n",
              " {0: 'pricerange', 1: 'not'},\n",
              " {0: 'stars', 1: 'not'},\n",
              " {0: 'type', 1: 'not'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo9jdpj1Cg5H"
      },
      "source": [
        "def write_output(output,fn):\n",
        "    with open(data_directory + f'pred_{fn}.txt', 'w') as file:\n",
        "        for sub_aspect in output:\n",
        "            file.write('%s\\n' % sub_aspect)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt21gjJaPt8j"
      },
      "source": [
        "for i, task in enumerate(task_names):\n",
        "    output = []\n",
        "    preds = predict(model, test_loaders[i], i)\n",
        "    for pred in preds:\n",
        "        output.append(all_ind2lab[i][pred.item()])\n",
        "    write_output(output, task)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQSuiGjtF8nb",
        "outputId": "647637cc-867b-49f4-81f2-8e38c7dea2fc"
      },
      "source": [
        "task_names"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['area', 'food', 'internet', 'name', 'parking', 'pricerange', 'stars', 'type']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgZ7IaDUEWVf"
      },
      "source": [
        "## Combine step1 and step2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0y81ajwFVQd"
      },
      "source": [
        "constrains = {\"find_hotel\":[\"area\", \"internet\", \"name\", \"parking\", \"pricerange\", \"stars\", \"type\"], \"find_restaurant\":[\"area\", \"food\", \"name\", \"pricerange\"]}"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD12l53hA1FX"
      },
      "source": [
        "result = []\n",
        "with open(data_directory + \"hotel_res.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      result.append([line.strip()])\n",
        "\n",
        "for i, task in enumerate(task_names):\n",
        "    with open(data_directory + f\"pred_{task}.txt\", \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            sub_aspect = line.strip()\n",
        "            domain = result[i][0]\n",
        "            if sub_aspect in constrains[domain]:\n",
        "                result[i].append(domain[5:]+\"-\"+sub_aspect)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIrEDRxvMFfr"
      },
      "source": [
        "with open(data_directory+\"domain_aspect_pred.txt\", \"w\") as f:\n",
        "    f.writelines(\"%s\\n\" % i for i in [\"|\".join(i) for i in result])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m5Pd35MAYiq"
      },
      "source": [
        "## Solution\n",
        "rubric={accuracy:10,quality:5,efficiency:3}\n",
        "\n",
        "You will build a system that, when provided with an utterance, predicts the appropriate intent and slots in the format used in the provided answers. This is an open-ended problem and you may solve it however you like, with the following restrictions:\n",
        "\n",
        "* Your solution should include at least one of token-level prediction models used in Labs 1-3 of this course, i.e. you should make use of a CRF, an LSTM, or a BERT model. You may use multiple models.\n",
        "* You may use basic NLP tools (tokenizer, POS, parser) and unsupervised resources such as word embeddings, but you should NOT use an existing NER system, or any additional labeled data for this task.\n",
        "* Your solution should be appropriately decomposed into parts, and documented. This is a complex enough problem that you should have several functions. You may wrap things up into a single class if you like, but you don't have to.\n",
        "* Use the provided assert to test `dev_predicted`, the output of your complete model on the dev set, you will need to pass the assert to get full accuracy points. \n",
        "* Though you may use dev *accuracy* to guide the development of your model, you should not look at either utterances or answers for the dev (or the test) when developing your model. Limit your inspection of the data (e.g. for the purposes of error analysis) to the training set.\n",
        "\n",
        "Other things to consider:\n",
        "\n",
        "* You may want to build \"standard\" (non-sequential) ML classifiers for some aspects of this problem, but you don't have to!\n",
        "* You may want to use appropriate lexicons. You can build them yourself, or find some.\n",
        "* Rather than using statistical classifiers, you may want to use rule-based methods to solve some of the problems you're facing.\n",
        "* You should probably do regular error analysis, some kind of crossvalidation in the training set is a good approach for this, or you can create another (inspectable) internal dev set by splitting up the training set.\n",
        "* If you're looking for just a little bit more performance, don't forget to tune your hyperparameters!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el69aPcgAYiq"
      },
      "source": [
        "## Report\n",
        "rubric={raw:2,reasoning:3,writing:1}\n",
        "\n",
        "Describe your system, and discuss what your thinking about particular choices and any experiments you tried. Please talk about things you tried but didn't work, or things you thought of doing but didn't. Finally, discuss how each group member contributed to the project. As usual, there is an expectation that every group member will have made some significant contribution to the project. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPIXLJp6AYiq"
      },
      "source": [
        "## Submit to Kaggle \n",
        "rubric={accuracy:2}\n",
        "\n",
        "Run your system over the test data, and submit the result (in the same format as the train/dev answers) to the Kaggle competition. The competition is hosted [here](https://www.kaggle.com/c/mds-cl-2020-21-colx-563-lab-assignment-4). To get full points, you need to beat the public baseline. Use your capstone partner as your team name please!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-i8_3TxAYit"
      },
      "source": [
        "## Exercise: Kaggle competition (Optional)\n",
        "rubric={raw:2}\n",
        "\n",
        "As a team, compete to get the best result in the task. Since there are only 8 teams, the distribution of marks is a bit different than usual, only the top 3 groups will get bonus points. As usual, the rankings will be based on the score on the private leaderboard:\n",
        "\n",
        "\n",
        "- 1st place: 2\n",
        "- 2nd place: 1\n",
        "- 3rd place: 0.5"
      ]
    }
  ]
}